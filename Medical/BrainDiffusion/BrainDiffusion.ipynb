{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import make_grid, save_image\n",
        "import matplotlib.pyplot as plt\n",
        "import nibabel as nib\n",
        "from tqdm.notebook import tqdm  # Use notebook version for better display\n",
        "import random\n",
        "import copy\n",
        "import math\n",
        "from pathlib import Path\n",
        "from contextlib import nullcontext\n",
        "\n",
        "# Optional imports with fallbacks\n",
        "try:\n",
        "    from skimage import measure, metrics\n",
        "    HAVE_SKIMAGE = True\n",
        "except ImportError:\n",
        "    HAVE_SKIMAGE = False\n",
        "    print(\"Warning: scikit-image not installed. Some visualization features will be limited.\")\n",
        "\n",
        "try:\n",
        "    import plotly.graph_objects as go\n",
        "    HAVE_PLOTLY = True\n",
        "except ImportError:\n",
        "    HAVE_PLOTLY = False\n",
        "    print(\"Warning: plotly not installed. 3D visualizations will be limited.\")\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set reproducibility\n",
        "def set_seeds(seed=42):\n",
        "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    # Note: Setting to True may impact performance, but ensures reproducibility\n",
        "\n",
        "set_seeds()\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "logger.info(f\"Using device: {device}\")\n",
        "\n",
        "# Memory information\n",
        "if torch.cuda.is_available():\n",
        "    logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    logger.info(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "id": "zDRXPHfOXY9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BrainDiffusionConfig:\n",
        "    \"\"\"Configuration class for BrainDiffusion model.\n",
        "\n",
        "    This class centralizes all hyperparameters and settings in one place.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Data Parameters\n",
        "        self.volume_size = (64, 64, 64)  # 3D volume dimensions (D, H, W)\n",
        "        self.in_channels = 1             # Single channel for T1/T2 MRI\n",
        "        self.patch_size = None           # Set to use patch-based training (e.g., (32, 32, 32))\n",
        "\n",
        "        # Model Parameters\n",
        "        self.base_channels = 32          # Base channel count (smaller for 3D due to memory constraints)\n",
        "        self.channel_mults = (1, 2, 4, 8)  # Channel multipliers at each resolution\n",
        "        self.attention_resolutions = (8,) # At which resolutions to apply attention\n",
        "        self.num_res_blocks = 2          # Number of residual blocks per resolution\n",
        "        self.dropout = 0.1               # Dropout rate\n",
        "        self.time_dim = 256              # Dimension for time embedding\n",
        "\n",
        "        # Diffusion Parameters\n",
        "        self.num_diffusion_steps = 1000  # Number of diffusion timesteps\n",
        "        self.beta_start = 1e-4           # Starting noise schedule value\n",
        "        self.beta_end = 0.02             # Ending noise schedule value\n",
        "\n",
        "        # Training Parameters\n",
        "        self.batch_size = 2              # Small batch size for 3D (memory constraints)\n",
        "        self.learning_rate = 1e-4        # Learning rate\n",
        "        self.weight_decay = 1e-5         # Weight decay for regularization\n",
        "        self.epochs = 100                # Number of training epochs\n",
        "        self.ema_decay = 0.9999          # Exponential moving average decay\n",
        "        self.train_val_split = 0.8       # Train/validation split ratio\n",
        "\n",
        "        # Memory Optimization\n",
        "        self.use_checkpointing = True    # Use gradient checkpointing to save memory\n",
        "        self.mixed_precision = True      # Use mixed precision training\n",
        "\n",
        "        # Sampling Parameters\n",
        "        self.sampling_steps = 50         # Default sampling steps (for DDIM)\n",
        "        self.use_ddim = True             # Use DDIM for faster sampling (vs. DDPM)\n",
        "        self.ddim_sampling_eta = 0.0     # DDIM sampling parameter (0 = deterministic)\n",
        "        self.guidance_scale = 2.0        # Classifier-free guidance scale\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"String representation of the configuration.\"\"\"\n",
        "        return \"\\n\".join(f\"{key} = {value}\" for key, value in vars(self).items())\n",
        "\n",
        "    def update(self, **kwargs):\n",
        "        \"\"\"Update configuration parameters.\"\"\"\n",
        "        for key, value in kwargs.items():\n",
        "            if hasattr(self, key):\n",
        "                setattr(self, key, value)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown parameter: {key}\")\n",
        "        return self\n",
        "\n",
        "# Create default configuration\n",
        "config = BrainDiffusionConfig()\n",
        "print(\"Default configuration:\")\n",
        "print(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjXz2m4XXY79",
        "outputId": "68c26a34-087e-4478-e4eb-f64b0b397240"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default configuration:\n",
            "volume_size = (64, 64, 64)\n",
            "in_channels = 1\n",
            "patch_size = None\n",
            "base_channels = 32\n",
            "channel_mults = (1, 2, 4, 8)\n",
            "attention_resolutions = (8,)\n",
            "num_res_blocks = 2\n",
            "dropout = 0.1\n",
            "time_dim = 256\n",
            "num_diffusion_steps = 1000\n",
            "beta_start = 0.0001\n",
            "beta_end = 0.02\n",
            "batch_size = 2\n",
            "learning_rate = 0.0001\n",
            "weight_decay = 1e-05\n",
            "epochs = 100\n",
            "ema_decay = 0.9999\n",
            "train_val_split = 0.8\n",
            "use_checkpointing = True\n",
            "mixed_precision = True\n",
            "sampling_steps = 50\n",
            "use_ddim = True\n",
            "ddim_sampling_eta = 0.0\n",
            "guidance_scale = 2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BrainVolumeLoader:\n",
        "    \"\"\"Handles loading and preprocessing of 3D brain MRI volumes.\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir, target_size=(64, 64, 64), normalize=True):\n",
        "        \"\"\"Initialize the volume loader.\n",
        "\n",
        "        Args:\n",
        "            data_dir: Path to directory containing NIfTI files\n",
        "            target_size: Target dimensions for volumes (D, H, W)\n",
        "            normalize: Whether to normalize intensity values to [0, 1]\n",
        "        \"\"\"\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.target_size = target_size\n",
        "        self.normalize = normalize\n",
        "        logger.info(f\"Initialized volume loader with target size: {target_size}\")\n",
        "\n",
        "    def load_nifti_volume(self, file_path):\n",
        "        \"\"\"Load a NIfTI file and return as numpy array.\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to NIfTI file\n",
        "\n",
        "        Returns:\n",
        "            volume: Numpy array of volume data\n",
        "            affine: Affine transformation matrix\n",
        "        \"\"\"\n",
        "        img = nib.load(file_path)\n",
        "        return img.get_fdata(), img.affine\n",
        "\n",
        "    def resample_volume(self, volume, target_shape):\n",
        "        \"\"\"Resample volume to target shape.\n",
        "\n",
        "        Args:\n",
        "            volume: Input volume as numpy array\n",
        "            target_shape: Target shape (D, H, W)\n",
        "\n",
        "        Returns:\n",
        "            Resampled volume\n",
        "        \"\"\"\n",
        "        # Convert to tensor for resampling\n",
        "        tensor = torch.from_numpy(volume).float().unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        # Resample using trilinear interpolation\n",
        "        resampled = F.interpolate(\n",
        "            tensor,\n",
        "            size=target_shape,\n",
        "            mode='trilinear',\n",
        "            align_corners=False\n",
        "        )\n",
        "\n",
        "        return resampled.squeeze(0).squeeze(0).numpy()\n",
        "\n",
        "    def normalize_volume(self, volume):\n",
        "        \"\"\"Normalize volume intensities to [0, 1].\n",
        "\n",
        "        Args:\n",
        "            volume: Input volume\n",
        "\n",
        "        Returns:\n",
        "            Normalized volume\n",
        "        \"\"\"\n",
        "        min_val = np.min(volume)\n",
        "        max_val = np.max(volume)\n",
        "\n",
        "        if max_val > min_val:\n",
        "            normalized = (volume - min_val) / (max_val - min_val)\n",
        "        else:\n",
        "            normalized = volume * 0\n",
        "\n",
        "        return normalized\n",
        "\n",
        "    def preprocess_volume(self, volume):\n",
        "        \"\"\"Preprocess a 3D volume.\n",
        "\n",
        "        Args:\n",
        "            volume: Input volume\n",
        "\n",
        "        Returns:\n",
        "            Preprocessed volume as tensor\n",
        "        \"\"\"\n",
        "        # Resample to target dimensions if needed\n",
        "        if volume.shape != self.target_size:\n",
        "            volume = self.resample_volume(volume, self.target_size)\n",
        "\n",
        "        # Normalize intensities if requested\n",
        "        if self.normalize:\n",
        "            volume = self.normalize_volume(volume)\n",
        "\n",
        "        # Convert to tensor\n",
        "        tensor = torch.from_numpy(volume).float().unsqueeze(0)  # Add channel dimension\n",
        "\n",
        "        return tensor\n",
        "\n",
        "    def extract_region_from_path(self, file_path):\n",
        "        \"\"\"Extract brain region from filepath or use a default.\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to NIfTI file\n",
        "\n",
        "        Returns:\n",
        "            Region identifier (integer)\n",
        "        \"\"\"\n",
        "        # This is a placeholder - adapt based on your dataset organization\n",
        "        file_path_str = str(file_path).lower()\n",
        "\n",
        "        if \"frontal\" in file_path_str:\n",
        "            return 0  # Frontal lobe\n",
        "        elif \"parietal\" in file_path_str:\n",
        "            return 1  # Parietal lobe\n",
        "        elif \"temporal\" in file_path_str:\n",
        "            return 2  # Temporal lobe\n",
        "        elif \"occipital\" in file_path_str:\n",
        "            return 3  # Occipital lobe\n",
        "        else:\n",
        "            return 4  # Other/unknown\n",
        "\n",
        "    def load_dataset(self, limit=None):\n",
        "        \"\"\"Load all NIfTI files in the data directory.\n",
        "\n",
        "        Args:\n",
        "            limit: Optional limit on number of files to load\n",
        "\n",
        "        Returns:\n",
        "            volumes: List of preprocessed volumes as tensors\n",
        "            region_labels: List of region labels\n",
        "        \"\"\"\n",
        "        # Find all NIfTI files\n",
        "        nifti_files = list(self.data_dir.glob('**/*.nii.gz'))\n",
        "        nifti_files.extend(list(self.data_dir.glob('**/*.nii')))\n",
        "\n",
        "        if limit:\n",
        "            nifti_files = nifti_files[:limit]\n",
        "\n",
        "        logger.info(f\"Found {len(nifti_files)} NIfTI files\")\n",
        "\n",
        "        volumes = []\n",
        "        region_labels = []\n",
        "\n",
        "        for file_path in tqdm(nifti_files, desc=\"Loading 3D volumes\"):\n",
        "            try:\n",
        "                # Get brain region\n",
        "                region = self.extract_region_from_path(file_path)\n",
        "\n",
        "                # Load and preprocess volume\n",
        "                volume_data, _ = self.load_nifti_volume(file_path)\n",
        "                processed_volume = self.preprocess_volume(volume_data)\n",
        "\n",
        "                volumes.append(processed_volume)\n",
        "                region_labels.append(region)\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "        logger.info(f\"Successfully loaded {len(volumes)} volumes\")\n",
        "        return volumes, region_labels"
      ],
      "metadata": {
        "id": "vxwh_rAUXh_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BrainVolumeAugmentation:\n",
        "    \"\"\"3D data augmentation techniques for brain volumes.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def random_flip(volume, axis=0, p=0.5):\n",
        "        \"\"\"Randomly flip volume along specified axis.\n",
        "\n",
        "        Args:\n",
        "            volume: Input volume [C, D, H, W]\n",
        "            axis: Axis to flip (0=depth, 1=height, 2=width)\n",
        "            p: Probability of applying flip\n",
        "\n",
        "        Returns:\n",
        "            Augmented volume\n",
        "        \"\"\"\n",
        "        if random.random() < p:\n",
        "            # Add 1 to axis because the first dimension is the channel\n",
        "            return torch.flip(volume, dims=[axis+1])\n",
        "        return volume\n",
        "\n",
        "    @staticmethod\n",
        "    def random_rotate_90(volume, axes=(1, 2), p=0.5):\n",
        "        \"\"\"Randomly rotate volume 90 degrees around specified axes.\n",
        "\n",
        "        Args:\n",
        "            volume: Input volume [C, D, H, W]\n",
        "            axes: Tuple of axes to rotate around (after channel dim)\n",
        "            p: Probability of applying rotation\n",
        "\n",
        "        Returns:\n",
        "            Augmented volume\n",
        "        \"\"\"\n",
        "        if random.random() < p:\n",
        "            k = random.randint(1, 3)  # 1, 2, or 3 times 90 degrees\n",
        "            # Add 1 to axes because the first dimension is the channel\n",
        "            adjusted_axes = (axes[0]+1, axes[1]+1)\n",
        "            return torch.rot90(volume, k=k, dims=adjusted_axes)\n",
        "        return volume\n",
        "\n",
        "    @staticmethod\n",
        "    def random_intensity_shift(volume, max_offset=0.1, p=0.5):\n",
        "        \"\"\"Randomly shift intensity values.\n",
        "\n",
        "        Args:\n",
        "            volume: Input volume [C, D, H, W]\n",
        "            max_offset: Maximum intensity offset\n",
        "            p: Probability of applying shift\n",
        "\n",
        "        Returns:\n",
        "            Augmented volume\n",
        "        \"\"\"\n",
        "        if random.random() < p:\n",
        "            offset = random.uniform(-max_offset, max_offset)\n",
        "            shifted = volume + offset\n",
        "            return torch.clamp(shifted, 0, 1)\n",
        "        return volume\n",
        "\n",
        "    @staticmethod\n",
        "    def random_intensity_scale(volume, min_scale=0.9, max_scale=1.1, p=0.5):\n",
        "        \"\"\"Randomly scale intensity values.\n",
        "\n",
        "        Args:\n",
        "            volume: Input volume [C, D, H, W]\n",
        "            min_scale: Minimum scaling factor\n",
        "            max_scale: Maximum scaling factor\n",
        "            p: Probability of applying scaling\n",
        "\n",
        "        Returns:\n",
        "            Augmented volume\n",
        "        \"\"\"\n",
        "        if random.random() < p:\n",
        "            scale = random.uniform(min_scale, max_scale)\n",
        "            scaled = volume * scale\n",
        "            return torch.clamp(scaled, 0, 1)\n",
        "        return volume\n",
        "\n",
        "    @staticmethod\n",
        "    def random_gaussian_noise(volume, std=0.01, p=0.5):\n",
        "        \"\"\"Add random Gaussian noise.\n",
        "\n",
        "        Args:\n",
        "            volume: Input volume [C, D, H, W]\n",
        "            std: Standard deviation of noise\n",
        "            p: Probability of adding noise\n",
        "\n",
        "        Returns:\n",
        "            Augmented volume\n",
        "        \"\"\"\n",
        "        if random.random() < p:\n",
        "            noise = torch.randn_like(volume) * std\n",
        "            noisy = volume + noise\n",
        "            return torch.clamp(noisy, 0, 1)\n",
        "        return volume\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_augmentations(volume, p=0.8):\n",
        "        \"\"\"Apply a series of random augmentations with probability p.\n",
        "\n",
        "        Args:\n",
        "            volume: Input volume [C, D, H, W]\n",
        "            p: Overall probability of applying augmentation\n",
        "\n",
        "        Returns:\n",
        "            Augmented volume\n",
        "        \"\"\"\n",
        "        if random.random() < p:\n",
        "            # Apply multiple augmentations\n",
        "            augs = [\n",
        "                lambda v: BrainVolumeAugmentation.random_flip(v, axis=0),\n",
        "                lambda v: BrainVolumeAugmentation.random_flip(v, axis=1),\n",
        "                lambda v: BrainVolumeAugmentation.random_flip(v, axis=2),\n",
        "                lambda v: BrainVolumeAugmentation.random_rotate_90(v, axes=(0, 1)),\n",
        "                lambda v: BrainVolumeAugmentation.random_rotate_90(v, axes=(0, 2)),\n",
        "                lambda v: BrainVolumeAugmentation.random_rotate_90(v, axes=(1, 2)),\n",
        "                lambda v: BrainVolumeAugmentation.random_intensity_shift(v),\n",
        "                lambda v: BrainVolumeAugmentation.random_intensity_scale(v),\n",
        "                lambda v: BrainVolumeAugmentation.random_gaussian_noise(v)\n",
        "            ]\n",
        "\n",
        "            # Apply 1-3 random augmentations\n",
        "            num_augs = random.randint(1, 3)\n",
        "            selected_augs = random.sample(augs, num_augs)\n",
        "\n",
        "            for aug in selected_augs:\n",
        "                volume = aug(volume)\n",
        "\n",
        "        return volume"
      ],
      "metadata": {
        "id": "JkGPZYm1XnN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BrainVolumeDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for 3D brain volumes with conditional region labels.\"\"\"\n",
        "\n",
        "    def __init__(self, volumes, region_labels=None, transform=None, patch_size=None):\n",
        "        \"\"\"Initialize the dataset.\n",
        "\n",
        "        Args:\n",
        "            volumes: List of volume tensors [C, D, H, W]\n",
        "            region_labels: Optional list of region labels\n",
        "            transform: Optional transform function\n",
        "            patch_size: Optional patch size for patch-based extraction\n",
        "        \"\"\"\n",
        "        self.volumes = volumes\n",
        "        self.region_labels = region_labels\n",
        "        self.transform = transform\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # Calculate number of unique regions for embedding dimension\n",
        "        if region_labels is not None:\n",
        "            self.num_regions = len(set(region_labels))\n",
        "            logger.info(f\"Dataset has {len(volumes)} volumes with {self.num_regions} regions\")\n",
        "        else:\n",
        "            self.num_regions = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of volumes in the dataset.\"\"\"\n",
        "        return len(self.volumes)\n",
        "\n",
        "    def extract_random_patch(self, volume):\n",
        "        \"\"\"Extract a random patch from the volume.\n",
        "\n",
        "        Args:\n",
        "            volume: Input volume [C, D, H, W]\n",
        "\n",
        "        Returns:\n",
        "            Extracted patch\n",
        "        \"\"\"\n",
        "        if self.patch_size is None:\n",
        "            return volume\n",
        "\n",
        "        c, d, h, w = volume.shape\n",
        "        pd, ph, pw = self.patch_size\n",
        "\n",
        "        # Randomly select patch origin\n",
        "        d_start = random.randint(0, d - pd) if d > pd else 0\n",
        "        h_start = random.randint(0, h - ph) if h > ph else 0\n",
        "        w_start = random.randint(0, w - pw) if w > pw else 0\n",
        "\n",
        "        # Extract patch\n",
        "        patch = volume[:, d_start:d_start+pd, h_start:h_start+ph, w_start:w_start+pw]\n",
        "\n",
        "        return patch\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Get a volume and its label.\n",
        "\n",
        "        Args:\n",
        "            idx: Index of the volume to retrieve\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing volume and region label\n",
        "        \"\"\"\n",
        "        volume = self.volumes[idx]\n",
        "\n",
        "        # Extract patch if needed\n",
        "        if self.patch_size is not None:\n",
        "            volume = self.extract_random_patch(volume)\n",
        "\n",
        "        # Apply transforms if provided\n",
        "        if self.transform:\n",
        "            volume = self.transform(volume)\n",
        "\n",
        "        # Get region label if available\n",
        "        region = -1  # Default for unconditional\n",
        "        if self.region_labels is not None:\n",
        "            region = self.region_labels[idx]\n",
        "\n",
        "        return {\n",
        "            'volume': volume,         # [C, D, H, W] tensor\n",
        "            'region': torch.tensor(region, dtype=torch.long)  # Class index\n",
        "        }\n",
        "\n",
        "\n",
        "def create_dataloaders(volumes, region_labels=None, config=None):\n",
        "    \"\"\"Create training and validation dataloaders for 3D volumes.\n",
        "\n",
        "    Args:\n",
        "        volumes: List of volume tensors\n",
        "        region_labels: Optional list of region labels\n",
        "        config: Configuration object\n",
        "\n",
        "    Returns:\n",
        "        train_loader: Training data loader\n",
        "        val_loader: Validation data loader\n",
        "        num_regions: Number of unique regions\n",
        "    \"\"\"\n",
        "    if config is None:\n",
        "        config = BrainDiffusionConfig()\n",
        "\n",
        "    # Create indices for train/val split\n",
        "    indices = list(range(len(volumes)))\n",
        "    random.shuffle(indices)\n",
        "    split = int(len(indices) * config.train_val_split)\n",
        "\n",
        "    train_indices = indices[:split]\n",
        "    val_indices = indices[split:]\n",
        "\n",
        "    # Split data\n",
        "    train_volumes = [volumes[i] for i in train_indices]\n",
        "    train_regions = None if region_labels is None else [region_labels[i] for i in train_indices]\n",
        "\n",
        "    val_volumes = [volumes[i] for i in val_indices]\n",
        "    val_regions = None if region_labels is None else [region_labels[i] for i in val_indices]\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = BrainVolumeDataset(\n",
        "        train_volumes,\n",
        "        train_regions,\n",
        "        transform=BrainVolumeAugmentation.apply_augmentations,\n",
        "        patch_size=config.patch_size\n",
        "    )\n",
        "\n",
        "    val_dataset = BrainVolumeDataset(\n",
        "        val_volumes,\n",
        "        val_regions,\n",
        "        patch_size=config.patch_size\n",
        "    )\n",
        "\n",
        "    # Get number of regions\n",
        "    num_regions = train_dataset.num_regions if hasattr(train_dataset, 'num_regions') else 0\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    logger.info(f\"Created dataloaders with {len(train_loader)} training batches and {len(val_loader)} validation batches\")\n",
        "\n",
        "    return train_loader, val_loader, num_regions"
      ],
      "metadata": {
        "id": "V8gmLdalXtkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiffusionModel3D:\n",
        "    \"\"\"Core diffusion model implementation for 3D volumes.\"\"\"\n",
        "\n",
        "    def __init__(self, config=None, device=device):\n",
        "        \"\"\"Initialize the diffusion process parameters.\n",
        "\n",
        "        Args:\n",
        "            config: Configuration object\n",
        "            device: Device to use for computations\n",
        "        \"\"\"\n",
        "        # Use default config if not provided\n",
        "        if config is None:\n",
        "            config = BrainDiffusionConfig()\n",
        "\n",
        "        self.num_diffusion_steps = config.num_diffusion_steps\n",
        "        self.beta_start = config.beta_start\n",
        "        self.beta_end = config.beta_end\n",
        "        self.device = device\n",
        "\n",
        "        # Define noise schedule (linear or cosine)\n",
        "        self.betas = self._linear_beta_schedule(\n",
        "            self.beta_start,\n",
        "            self.beta_end,\n",
        "            self.num_diffusion_steps\n",
        "        ).to(device)\n",
        "\n",
        "        # Precompute values for forward and reverse processes\n",
        "        self.alphas = 1.0 - self.betas\n",
        "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
        "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "        self.sqrt_recip_alphas = torch.sqrt(1.0 / self.alphas)\n",
        "\n",
        "        # Calculations for diffusion q(x_t | x_{t-1})\n",
        "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
        "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n",
        "\n",
        "        # Calculations for posterior q(x_{t-1} | x_t, x_0)\n",
        "        self.posterior_variance = self.betas * (1. - self.alphas_cumprod_prev) / (1. - self.alphas_cumprod)\n",
        "\n",
        "    def _linear_beta_schedule(self, beta_start, beta_end, num_diffusion_steps):\n",
        "        \"\"\"Linear noise schedule.\n",
        "\n",
        "        Args:\n",
        "            beta_start: Starting noise schedule value\n",
        "            beta_end: Ending noise schedule value\n",
        "            num_diffusion_steps: Number of diffusion steps\n",
        "\n",
        "        Returns:\n",
        "            Tensor of beta values\n",
        "        \"\"\"\n",
        "        return torch.linspace(beta_start, beta_end, num_diffusion_steps)\n",
        "\n",
        "    def _cosine_beta_schedule(self, num_diffusion_steps, s=0.008):\n",
        "        \"\"\"Cosine noise schedule as proposed in improved DDPM papers.\n",
        "\n",
        "        Args:\n",
        "            num_diffusion_steps: Number of diffusion steps\n",
        "            s: Offset parameter\n",
        "\n",
        "        Returns:\n",
        "            Tensor of beta values\n",
        "        \"\"\"\n",
        "        steps = num_diffusion_steps + 1\n",
        "        t = torch.linspace(0, num_diffusion_steps, steps) / num_diffusion_steps\n",
        "        alphas_cumprod = torch.cos((t + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
        "        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "        return torch.clip(betas, 0, 0.999)\n",
        "\n",
        "    def q_sample(self, x_0, t, noise=None):\n",
        "        \"\"\"Forward diffusion process: q(x_t | x_0)\n",
        "\n",
        "        Args:\n",
        "            x_0: Original clean volumes [B, C, D, H, W]\n",
        "            t: Diffusion timesteps [B]\n",
        "            noise: Optional pre-generated noise\n",
        "\n",
        "        Returns:\n",
        "            x_t: Noised volumes at timestep t\n",
        "            noise: The noise used\n",
        "        \"\"\"\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x_0)\n",
        "\n",
        "        # Get appropriate values for timestep t\n",
        "        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t].reshape(-1, 1, 1, 1, 1)\n",
        "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].reshape(-1, 1, 1, 1, 1)\n",
        "\n",
        "        # Forward process formula: q(x_t | x_0) = sqrt(ɑt)x_0 + sqrt(1-ɑt)ε\n",
        "        x_t = sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise\n",
        "\n",
        "        return x_t, noise\n",
        "\n",
        "    def compute_loss(self, model, x_0, condition, t=None):\n",
        "        \"\"\"Compute training loss for the denoising model.\n",
        "\n",
        "        Args:\n",
        "            model: Denoising U-Net model\n",
        "            x_0: Original clean volumes [B, C, D, H, W]\n",
        "            condition: Conditioning information\n",
        "            t: Optional specific timesteps, otherwise random\n",
        "\n",
        "        Returns:\n",
        "            Loss value\n",
        "        \"\"\"\n",
        "        B = x_0.shape[0]\n",
        "\n",
        "        # Sample random timesteps if not provided\n",
        "        if t is None:\n",
        "            t = torch.randint(0, self.num_diffusion_steps, (B,), device=self.device)\n",
        "\n",
        "        # Forward process to get noisy volumes x_t\n",
        "        x_t, noise = self.q_sample(x_0, t)\n",
        "\n",
        "        # Predict the noise using the model\n",
        "        noise_pred = model(x_t, t, condition)\n",
        "\n",
        "        # Simple MSE loss between actual and predicted noise\n",
        "        loss = F.mse_loss(noise_pred, noise)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "vooS8InaXwH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extend DiffusionModel3D with sampling methods\n",
        "\n",
        "def p_sample(self, model, x_t, t, condition, guidance_scale=1.0):\n",
        "    \"\"\"Single step of the reverse diffusion sampling process.\n",
        "\n",
        "    Args:\n",
        "        model: Denoising model\n",
        "        x_t: Noisy volume at timestep t\n",
        "        t: Current timestep\n",
        "        condition: Conditioning information\n",
        "        guidance_scale: Scale for classifier-free guidance\n",
        "\n",
        "    Returns:\n",
        "        Denoised sample for timestep t-1\n",
        "    \"\"\"\n",
        "    # Get beta and alpha values for timestep t\n",
        "    betas_t = self.betas[t]\n",
        "    sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t]\n",
        "    sqrt_recip_alphas_t = self.sqrt_recip_alphas[t]\n",
        "\n",
        "    # Model prediction with guidance if requested\n",
        "    if guidance_scale > 1.0 and condition is not None:\n",
        "        # Predict with conditioning\n",
        "        noise_pred_cond = model(x_t, t, condition)\n",
        "\n",
        "        # Predict without conditioning (unconditional)\n",
        "        noise_pred_uncond = model(x_t, t, None)\n",
        "\n",
        "        # Apply classifier-free guidance\n",
        "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
        "    else:\n",
        "        # Regular conditional or unconditional generation\n",
        "        noise_pred = model(x_t, t, condition)\n",
        "\n",
        "    # Algorithm 2 from DDPM paper for p(x_{t-1} | x_t)\n",
        "    model_mean = sqrt_recip_alphas_t.reshape(-1, 1, 1, 1, 1) * (\n",
        "        x_t - betas_t.reshape(-1, 1, 1, 1, 1) * noise_pred /\n",
        "        sqrt_one_minus_alphas_cumprod_t.reshape(-1, 1, 1, 1, 1)\n",
        "    )\n",
        "\n",
        "    if t[0] > 0:\n",
        "        # Add noise only if not the final step\n",
        "        posterior_variance_t = self.posterior_variance[t].reshape(-1, 1, 1, 1, 1)\n",
        "        noise = torch.randn_like(x_t)\n",
        "        return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
        "    else:\n",
        "        # For t=0, don't add noise\n",
        "        return model_mean\n",
        "\n",
        "def p_sample_loop(self, model, shape, condition, n_steps=None, guidance_scale=1.0):\n",
        "    \"\"\"Full reverse diffusion sampling loop to generate new volumes.\n",
        "\n",
        "    Args:\n",
        "        model: Denoising model\n",
        "        shape: Shape of volumes to generate [B, C, D, H, W]\n",
        "        condition: Conditioning information\n",
        "        n_steps: Optional number of steps (defaults to full process)\n",
        "        guidance_scale: Scale for classifier-free guidance\n",
        "\n",
        "    Returns:\n",
        "        Generated volumes and intermediate steps\n",
        "    \"\"\"\n",
        "    B = shape[0]\n",
        "    if n_steps is None:\n",
        "        n_steps = self.num_diffusion_steps\n",
        "\n",
        "    # Start from pure noise\n",
        "    x = torch.randn(shape, device=self.device)\n",
        "\n",
        "    # Track intermediate generations for visualization\n",
        "    intermediates = []\n",
        "\n",
        "    # Progress bar for sampling\n",
        "    progress_bar = tqdm(reversed(range(0, n_steps)), desc='Sampling', total=n_steps)\n",
        "\n",
        "    for i in progress_bar:\n",
        "        # For each batch element, use same timestep\n",
        "        t = torch.full((B,), i, device=self.device, dtype=torch.long)\n",
        "\n",
        "        # Single step of denoising\n",
        "        x = self.p_sample(model, x, t, condition, guidance_scale=guidance_scale)\n",
        "\n",
        "        # Save intermediate results every few steps\n",
        "        if i % (n_steps // 10) == 0 or i == n_steps - 1:\n",
        "            intermediates.append(x.detach().cpu())\n",
        "\n",
        "    return x, intermediates\n",
        "\n",
        "def ddim_sample(self, model, shape, condition, n_steps=50, guidance_scale=1.0, eta=0.0):\n",
        "    \"\"\"DDIM sampling for faster and deterministic generation.\n",
        "\n",
        "    Args:\n",
        "        model: Denoising model\n",
        "        shape: Shape of volumes to generate [B, C, D, H, W]\n",
        "        condition: Conditioning information\n",
        "        n_steps: Number of DDIM steps (typically much less than DDPM)\n",
        "        guidance_scale: Scale for classifier-free guidance\n",
        "        eta: DDIM stochasticity parameter (0 = deterministic, 1 = DDPM-like)\n",
        "\n",
        "    Returns:\n",
        "        Generated volumes and intermediate steps\n",
        "    \"\"\"\n",
        "    # Implementation follows DDIM paper: https://arxiv.org/abs/2010.02502\n",
        "    B = shape[0]\n",
        "\n",
        "    # Subsample original diffusion steps for DDIM\n",
        "    step_indices = torch.linspace(0, self.num_diffusion_steps - 1, n_steps,\n",
        "                                 dtype=torch.long, device=self.device)\n",
        "    alphas_cumprod_sub = self.alphas_cumprod[step_indices]\n",
        "\n",
        "    # Start from pure noise\n",
        "    x = torch.randn(shape, device=self.device)\n",
        "\n",
        "    intermediates = []\n",
        "\n",
        "    # DDIM sampling loop\n",
        "    progress_bar = tqdm(reversed(range(0, n_steps)), desc='DDIM Sampling', total=n_steps)\n",
        "\n",
        "    for i in progress_bar:\n",
        "        # Current timestep\n",
        "        t = torch.full((B,), step_indices[i], device=self.device, dtype=torch.long)\n",
        "\n",
        "        # Current α_t\n",
        "        alpha_cumprod_t = alphas_cumprod_sub[i]\n",
        "\n",
        "        # Previous α_{t-1} (or 1.0 for final step)\n",
        "        alpha_cumprod_prev = alphas_cumprod_sub[i-1] if i > 0 else torch.ones_like(alpha_cumprod_t)\n",
        "\n",
        "        # Model prediction with guidance if requested\n",
        "        if guidance_scale > 1.0 and condition is not None:\n",
        "            # Predict with and without conditioning\n",
        "            noise_pred_cond = model(x, t, condition)\n",
        "            noise_pred_uncond = model(x, t, None)\n",
        "\n",
        "            # Apply classifier-free guidance\n",
        "            predicted_noise = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
        "        else:\n",
        "            predicted_noise = model(x, t, condition)\n",
        "\n",
        "        # DDIM deterministic sampling formula\n",
        "        # Extract x_0 from x_t and predicted noise\n",
        "        sqrt_alpha_cumprod_t = torch.sqrt(alpha_cumprod_t)\n",
        "        sqrt_one_minus_alpha_cumprod_t = torch.sqrt(1 - alpha_cumprod_t)\n",
        "\n",
        "        # Predict x_0 from x_t and noise\n",
        "        x_0_pred = (x - sqrt_one_minus_alpha_cumprod_t.reshape(-1, 1, 1, 1, 1) * predicted_noise) / \\\n",
        "                  sqrt_alpha_cumprod_t.reshape(-1, 1, 1, 1, 1)\n",
        "\n",
        "        # For final step, just return the predicted x_0\n",
        "        if i == 0:\n",
        "            x = x_0_pred\n",
        "        else:\n",
        "            # DDIM update\n",
        "            sqrt_alpha_cumprod_prev = torch.sqrt(alpha_cumprod_prev)\n",
        "            sqrt_one_minus_alpha_cumprod_prev = torch.sqrt(1 - alpha_cumprod_prev)\n",
        "\n",
        "            # Add noise based on eta parameter\n",
        "            sigma_t = eta * torch.sqrt(\n",
        "                (1 - alpha_cumprod_prev) / (1 - alpha_cumprod_t) *\n",
        "                (1 - alpha_cumprod_t / alpha_cumprod_prev)\n",
        "            )\n",
        "\n",
        "            # Sample from distribution\n",
        "            if eta > 0:\n",
        "                noise = torch.randn_like(x)\n",
        "                x = sqrt_alpha_cumprod_prev.reshape(-1, 1, 1, 1, 1) * x_0_pred + \\\n",
        "                    sqrt_one_minus_alpha_cumprod_prev.reshape(-1, 1, 1, 1, 1) * predicted_noise + \\\n",
        "                    sigma_t.reshape(-1, 1, 1, 1, 1) * noise\n",
        "            else:\n",
        "                # Deterministic (eta = 0)\n",
        "                x = sqrt_alpha_cumprod_prev.reshape(-1, 1, 1, 1, 1) * x_0_pred + \\\n",
        "                    sqrt_one_minus_alpha_cumprod_prev.reshape(-1, 1, 1, 1, 1) * predicted_noise\n",
        "\n",
        "        # Save intermediate results\n",
        "        if i % (n_steps // 5) == 0 or i == n_steps - 1:\n",
        "            intermediates.append(x.detach().cpu())\n",
        "\n",
        "    return x, intermediates\n",
        "\n",
        "# Add method implementations to the class\n",
        "DiffusionModel3D.p_sample = p_sample\n",
        "DiffusionModel3D.p_sample_loop = p_sample_loop\n",
        "DiffusionModel3D.ddim_sample = ddim_sample"
      ],
      "metadata": {
        "id": "JUe8vfS8Xzwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeEmbedding(nn.Module):\n",
        "    \"\"\"Time step embedding module.\"\"\"\n",
        "\n",
        "    def __init__(self, dim):\n",
        "        \"\"\"Initialize time embedding module.\n",
        "\n",
        "        Args:\n",
        "            dim: Embedding dimension\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "        # First linear layer to expand timestep\n",
        "        self.linear_1 = nn.Linear(dim, dim * 4)\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        # Second linear layer\n",
        "        self.linear_2 = nn.Linear(dim * 4, dim * 4)\n",
        "\n",
        "    def forward(self, t):\n",
        "        \"\"\"Forward pass through time embedding.\n",
        "\n",
        "        Args:\n",
        "            t: Timesteps [B] or [B, 1]\n",
        "\n",
        "        Returns:\n",
        "            Time embeddings\n",
        "        \"\"\"\n",
        "        # Ensure t has correct shape\n",
        "        if len(t.shape) == 1:\n",
        "            t = t.unsqueeze(-1)\n",
        "\n",
        "        # First convert to embedding using sinusoidal positions\n",
        "        half_dim = self.dim // 2\n",
        "        emb = torch.log(torch.tensor(10000.0, device=t.device)) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
        "        emb = t * emb\n",
        "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
        "\n",
        "        # Up-project embedding\n",
        "        emb = self.linear_1(emb)\n",
        "        emb = self.act(emb)\n",
        "        emb = self.linear_2(emb)\n",
        "\n",
        "        return emb\n",
        "\n",
        "\n",
        "class ConditionalEmbedding(nn.Module):\n",
        "    \"\"\"Embedding for conditioning information (e.g., brain region).\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, dim):\n",
        "        \"\"\"Initialize conditional embedding module.\n",
        "\n",
        "        Args:\n",
        "            num_classes: Number of classes to embed\n",
        "            dim: Embedding dimension\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "        # Standard embedding layer\n",
        "        self.embedding = nn.Embedding(num_classes, dim)\n",
        "\n",
        "        # Projection layers\n",
        "        self.linear_1 = nn.Linear(dim, dim * 4)\n",
        "        self.act = nn.SiLU()\n",
        "        self.linear_2 = nn.Linear(dim * 4, dim * 4)\n",
        "\n",
        "    def forward(self, condition):\n",
        "        \"\"\"Forward pass through condition embedding.\n",
        "\n",
        "        Args:\n",
        "            condition: Conditioning class indices [B]\n",
        "\n",
        "        Returns:\n",
        "            Condition embeddings, or zeros for unconditional\n",
        "        \"\"\"\n",
        "        if condition is None:\n",
        "            # Return zeros for unconditional generation\n",
        "            # Useful for classifier-free guidance\n",
        "            batch_size = 1  # Fallback value\n",
        "            return torch.zeros(batch_size, self.dim * 4, device=self.embedding.weight.device)\n",
        "\n",
        "        # Get embeddings\n",
        "        emb = self.embedding(condition)\n",
        "        emb = self.linear_1(emb)\n",
        "        emb = self.act(emb)\n",
        "        emb = self.linear_2(emb)\n",
        "\n",
        "        return emb\n",
        "\n",
        "\n",
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    \"\"\"Sinusoidal position embeddings for 3D volumes.\"\"\"\n",
        "\n",
        "    def __init__(self, dim):\n",
        "        \"\"\"Initialize sinusoidal embedding module.\n",
        "\n",
        "        Args:\n",
        "            dim: Embedding dimension\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        \"\"\"Forward pass through sinusoidal embedding.\n",
        "\n",
        "        Args:\n",
        "            time: Time values [B]\n",
        "\n",
        "        Returns:\n",
        "            Time embeddings\n",
        "        \"\"\"\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=time.device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "6Nz5ioKIYL9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock3D(nn.Module):\n",
        "    \"\"\"Residual block for 3D U-Net with conditioning and time embedding.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, time_dim, use_attention=False, dropout=0.1, use_checkpoint=True):\n",
        "        \"\"\"Initialize residual block.\n",
        "\n",
        "        Args:\n",
        "            in_channels: Number of input channels\n",
        "            out_channels: Number of output channels\n",
        "            time_dim: Time embedding dimension\n",
        "            use_attention: Whether to use self-attention\n",
        "            dropout: Dropout probability\n",
        "            use_checkpoint: Whether to use gradient checkpointing\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "\n",
        "        # First conv block\n",
        "        self.norm1 = nn.GroupNorm(8, in_channels)\n",
        "        self.act1 = nn.SiLU()\n",
        "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "\n",
        "        # Time and condition projection\n",
        "        self.time_proj = nn.Linear(time_dim, out_channels)\n",
        "\n",
        "        # Second conv block\n",
        "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
        "        self.act2 = nn.SiLU()\n",
        "        self.dropout = nn.Dropout3d(dropout) if dropout > 0 else nn.Identity()\n",
        "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "\n",
        "        # Residual connection if channel dimensions don't match\n",
        "        self.residual_conv = nn.Conv3d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()\n",
        "\n",
        "        # Optional attention layer\n",
        "        self.use_attention = use_attention\n",
        "        if use_attention:\n",
        "            self.attention = SelfAttention3D(out_channels)\n",
        "\n",
        "    def _forward_impl(self, x, time_emb, cond_emb=None):\n",
        "        \"\"\"Forward implementation without checkpointing.\n",
        "\n",
        "        Args:\n",
        "            x: Input feature maps [B, C, D, H, W]\n",
        "            time_emb: Time embedding [B, time_dim*4]\n",
        "            cond_emb: Conditional embedding [B, time_dim*4]\n",
        "\n",
        "        Returns:\n",
        "            Output feature maps\n",
        "        \"\"\"\n",
        "        # Residual branch\n",
        "        residual = self.residual_conv(x)\n",
        "\n",
        "        # Main branch\n",
        "        h = self.norm1(x)\n",
        "        h = self.act1(h)\n",
        "        h = self.conv1(h)\n",
        "\n",
        "        # Add time embedding\n",
        "        time_emb = self.time_proj(time_emb[:, :time_emb.shape[1]//4])\n",
        "        h = h + time_emb.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "        # Add conditional embedding if provided\n",
        "        if cond_emb is not None:\n",
        "            # Assuming same dimension as time embedding for simplicity\n",
        "            cond_emb = self.time_proj(cond_emb[:, :cond_emb.shape[1]//4])\n",
        "            h = h + cond_emb.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "        h = self.norm2(h)\n",
        "        h = self.act2(h)\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv2(h)\n",
        "\n",
        "        # Apply attention if needed\n",
        "        if self.use_attention:\n",
        "            h = self.attention(h)\n",
        "\n",
        "        # Add residual connection\n",
        "        return h + residual\n",
        "\n",
        "    def forward(self, x, time_emb, cond_emb=None):\n",
        "        \"\"\"Forward pass with optional gradient checkpointing.\n",
        "\n",
        "        Args:\n",
        "            x: Input feature maps [B, C, D, H, W]\n",
        "            time_emb: Time embedding [B, time_dim*4]\n",
        "            cond_emb: Conditional embedding [B, time_dim*4]\n",
        "\n",
        "        Returns:\n",
        "            Output feature maps\n",
        "        \"\"\"\n",
        "        if self.use_checkpoint and self.training:\n",
        "            # Use gradient checkpointing to save memory\n",
        "            from torch.utils.checkpoint import checkpoint\n",
        "            return checkpoint(\n",
        "                lambda x, t, c: self._forward_impl(x, t, c),\n",
        "                x, time_emb, cond_emb\n",
        "            )\n",
        "        else:\n",
        "            return self._forward_impl(x, time_emb, cond_emb)\n",
        "\n",
        "\n",
        "class SelfAttention3D(nn.Module):\n",
        "    \"\"\"Self-attention module for 3D U-Net.\"\"\"\n",
        "\n",
        "    def __init__(self, channels, attention_heads=4):\n",
        "        \"\"\"Initialize self-attention module.\n",
        "\n",
        "        Args:\n",
        "            channels: Number of input channels\n",
        "            attention_heads: Number of attention heads\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.attention_heads = attention_heads\n",
        "\n",
        "        # Normalization and projections\n",
        "        self.norm = nn.GroupNorm(8, channels)\n",
        "        self.qkv = nn.Conv3d(channels, channels * 3, kernel_size=1)\n",
        "        self.proj = nn.Conv3d(channels, channels, kernel_size=1)\n",
        "\n",
        "        # Split multihead attention\n",
        "        self.head_dim = channels // attention_heads\n",
        "        self.scale = (self.head_dim) ** -0.5\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through self-attention.\n",
        "\n",
        "        Args:\n",
        "            x: Input feature maps [B, C, D, H, W]\n",
        "\n",
        "        Returns:\n",
        "            Attention-enhanced feature maps\n",
        "        \"\"\"\n",
        "        B, C, D, H, W = x.shape\n",
        "\n",
        "        # Normalize input\n",
        "        h = self.norm(x)\n",
        "\n",
        "        # Get q, k, v projections\n",
        "        qkv = self.qkv(h)\n",
        "        q, k, v = torch.chunk(qkv, 3, dim=1)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        q = q.reshape(B, self.attention_heads, self.head_dim, -1).permute(0, 1, 3, 2)\n",
        "        k = k.reshape(B, self.attention_heads, self.head_dim, -1)\n",
        "        v = v.reshape(B, self.attention_heads, self.head_dim, -1).permute(0, 1, 3, 2)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attention = torch.matmul(q, k) * self.scale  # [B, heads, DHW, DHW]\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "\n",
        "        # Apply attention to values\n",
        "        h = torch.matmul(attention, v).permute(0, 1, 3, 2)\n",
        "        h = h.reshape(B, C, D, H, W)\n",
        "\n",
        "        # Final projection\n",
        "        h = self.proj(h)\n",
        "\n",
        "        return h + x\n",
        "\n",
        "\n",
        "class DownsampleBlock3D(nn.Module):\n",
        "    \"\"\"Downsample block for 3D U-Net.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, time_dim, use_attention=False, dropout=0.1, use_checkpoint=True):\n",
        "        \"\"\"Initialize downsample block.\n",
        "\n",
        "        Args:\n",
        "            in_channels: Number of input channels\n",
        "            out_channels: Number of output channels\n",
        "            time_dim: Time embedding dimension\n",
        "            use_attention: Whether to use self-attention\n",
        "            dropout: Dropout probability\n",
        "            use_checkpoint: Whether to use gradient checkpointing\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Two residual blocks\n",
        "        self.res1 = ResidualBlock3D(in_channels, out_channels, time_dim, use_attention, dropout, use_checkpoint)\n",
        "        self.res2 = ResidualBlock3D(out_channels, out_channels, time_dim, use_attention, dropout, use_checkpoint)\n",
        "\n",
        "        # Downsample using strided convolution\n",
        "        self.downsample = nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x, time_emb, cond_emb=None):\n",
        "        \"\"\"Forward pass through downsample block.\n",
        "\n",
        "        Args:\n",
        "            x: Input feature maps\n",
        "            time_emb: Time embedding\n",
        "            cond_emb: Conditional embedding\n",
        "\n",
        "        Returns:\n",
        "            Downsampled features and skip connection\n",
        "        \"\"\"\n",
        "        x = self.res1(x, time_emb, cond_emb)\n",
        "        x = self.res2(x, time_emb, cond_emb)\n",
        "        return self.downsample(x), x\n",
        "\n",
        "\n",
        "class UpsampleBlock3D(nn.Module):\n",
        "    \"\"\"Upsample block for 3D U-Net.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, time_dim, use_attention=False, dropout=0.1, use_checkpoint=True):\n",
        "        \"\"\"Initialize upsample block.\n",
        "\n",
        "        Args:\n",
        "            in_channels: Number of input channels\n",
        "            out_channels: Number of output channels\n",
        "            time_dim: Time embedding dimension\n",
        "            use_attention: Whether to use self-attention\n",
        "            dropout: Dropout probability\n",
        "            use_checkpoint: Whether to use gradient checkpointing\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Two residual blocks with skip connection input\n",
        "        self.res1 = ResidualBlock3D(in_channels + out_channels, out_channels, time_dim, use_attention, dropout, use_checkpoint)\n",
        "        self.res2 = ResidualBlock3D(out_channels, out_channels, time_dim, use_attention, dropout, use_checkpoint)\n",
        "\n",
        "        # Upsample using transposed convolution\n",
        "        self.upsample = nn.ConvTranspose3d(out_channels, out_channels, kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x, skip, time_emb, cond_emb=None):\n",
        "        \"\"\"Forward pass through upsample block.\n",
        "\n",
        "        Args:\n",
        "            x: Input feature maps\n",
        "            skip: Skip connection from encoder\n",
        "            time_emb: Time embedding\n",
        "            cond_emb: Conditional embedding\n",
        "\n",
        "        Returns:\n",
        "            Upsampled feature maps\n",
        "        \"\"\"\n",
        "        x = self.upsample(x)\n",
        "\n",
        "        # Handle potential size mismatches by padding/cropping\n",
        "        if x.shape[2:] != skip.shape[2:]:\n",
        "            x = F.interpolate(x, size=skip.shape[2:], mode='trilinear', align_corners=False)\n",
        "\n",
        "        # Concatenate with skip connection\n",
        "        x = torch.cat([x, skip], dim=1)\n",
        "\n",
        "        x = self.res1(x, time_emb, cond_emb)\n",
        "        x = self.res2(x, time_emb, cond_emb)\n",
        "        return x"
      ],
      "metadata": {
        "id": "FJeBKcrNYNS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet3D(nn.Module):\n",
        "    \"\"\"3D U-Net model for denoising diffusion process.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        config=None,\n",
        "        in_channels=1,\n",
        "        out_channels=1,\n",
        "        time_dim=256,\n",
        "        num_classes=None,\n",
        "        base_dim=32,\n",
        "        dim_mults=(1, 2, 4, 8),\n",
        "        attention_resolutions=(8,),\n",
        "        dropout=0.1,\n",
        "        use_checkpoint=True\n",
        "    ):\n",
        "        \"\"\"Initialize 3D U-Net model.\n",
        "\n",
        "        Args:\n",
        "            config: Optional configuration object\n",
        "            in_channels: Number of input channels\n",
        "            out_channels: Number of output channels\n",
        "            time_dim: Time embedding dimension\n",
        "            num_classes: Number of condition classes\n",
        "            base_dim: Base channel dimension\n",
        "            dim_mults: Channel multipliers at each resolution\n",
        "            attention_resolutions: At which resolutions to apply attention\n",
        "            dropout: Dropout probability\n",
        "            use_checkpoint: Whether to use gradient checkpointing\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Use config if provided\n",
        "        if config is not None:\n",
        "            base_dim = config.base_channels\n",
        "            dim_mults = config.channel_mults\n",
        "            dropout = config.dropout\n",
        "            use_checkpoint = config.use_checkpointing\n",
        "            time_dim = config.time_dim\n",
        "\n",
        "        # Dimensions at each resolution\n",
        "        dims = [base_dim * m for m in dim_mults]\n",
        "\n",
        "        # Initial projection\n",
        "        self.init_conv = nn.Conv3d(in_channels, base_dim, kernel_size=3, padding=1)\n",
        "\n",
        "        # Time embedding\n",
        "        self.time_embedding = TimeEmbedding(time_dim)\n",
        "\n",
        "        # Optional class conditioning\n",
        "        self.has_class_conditioning = num_classes is not None and num_classes > 0\n",
        "        if self.has_class_conditioning:\n",
        "            self.class_embedding = ConditionalEmbedding(num_classes, time_dim)\n",
        "\n",
        "        # Encoder part of U-Net (downsampling)\n",
        "        self.downs = nn.ModuleList([])\n",
        "        in_dim = base_dim\n",
        "\n",
        "        # Current resolution relative to input\n",
        "        current_res = 1\n",
        "        resolutions = [current_res]\n",
        "\n",
        "        for i, dim in enumerate(dims):\n",
        "            # Determine if we use attention at this resolution\n",
        "            use_attention = current_res in attention_resolutions\n",
        "\n",
        "            # Add downsample block\n",
        "            self.downs.append(\n",
        "                DownsampleBlock3D(\n",
        "                    in_dim, dim, time_dim,\n",
        "                    use_attention=use_attention,\n",
        "                    dropout=dropout,\n",
        "                    use_checkpoint=use_checkpoint\n",
        "                )\n",
        "            )\n",
        "\n",
        "            in_dim = dim\n",
        "            current_res *= 2\n",
        "            resolutions.append(current_res)\n",
        "\n",
        "        # Middle part of U-Net (bottleneck)\n",
        "        self.mid = nn.ModuleList([\n",
        "            ResidualBlock3D(dims[-1], dims[-1], time_dim,\n",
        "                           use_attention=True, dropout=dropout,\n",
        "                           use_checkpoint=use_checkpoint),\n",
        "            ResidualBlock3D(dims[-1], dims[-1], time_dim,\n",
        "                           use_attention=True, dropout=dropout,\n",
        "                           use_checkpoint=use_checkpoint)\n",
        "        ])\n",
        "\n",
        "        # Decoder part of U-Net (upsampling)\n",
        "        self.ups = nn.ModuleList([])\n",
        "\n",
        "        for i, dim in enumerate(reversed(dims)):\n",
        "            # Determine if we use attention at this resolution\n",
        "            use_attention = resolutions[-(i+2)] in attention_resolutions\n",
        "\n",
        "            # Add upsample block\n",
        "            self.ups.append(\n",
        "                UpsampleBlock3D(\n",
        "                    dim, dims[max(0, len(dims)-i-2)], time_dim,\n",
        "                    use_attention=use_attention,\n",
        "                    dropout=dropout,\n",
        "                    use_checkpoint=use_checkpoint\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Final output projection\n",
        "        self.final_conv = nn.Sequential(\n",
        "            nn.GroupNorm(8, base_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv3d(base_dim, out_channels, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        # Print model size estimate\n",
        "        self._initialize_weights()\n",
        "        self._print_model_size()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Initialize weights for better training stability.\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv3d, nn.Linear)):\n",
        "                torch.nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "    def _print_model_size(self):\n",
        "        \"\"\"Print model size information.\"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        n_trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        logger.info(f\"UNet3D model has {n_params:,} parameters ({n_trainable:,} trainable)\")\n",
        "\n",
        "    def forward(self, x, t, condition=None):\n",
        "        \"\"\"Forward pass through U-Net.\n",
        "\n",
        "        Args:\n",
        "            x: Input noisy volumes [B, C, D, H, W]\n",
        "            t: Noise timesteps [B]\n",
        "            condition: Optional conditioning information\n",
        "\n",
        "        Returns:\n",
        "            Predicted noise\n",
        "        \"\"\"\n",
        "        # Initial feature extraction\n",
        "        x = self.init_conv(x)\n",
        "\n",
        "        # Time embedding\n",
        "        t_emb = self.time_embedding(t)\n",
        "\n",
        "        # Class embedding (if using)\n",
        "        c_emb = None\n",
        "        if self.has_class_conditioning:\n",
        "            c_emb = self.class_embedding(condition)\n",
        "\n",
        "        # Store skip connections\n",
        "        skips = []\n",
        "\n",
        "        # Encoder/Downsampling path\n",
        "        for down in self.downs:\n",
        "            x, skip = down(x, t_emb, c_emb)\n",
        "            skips.append(skip)\n",
        "\n",
        "        # Middle/Bottleneck\n",
        "        for mid_block in self.mid:\n",
        "            x = mid_block(x, t_emb, c_emb)\n",
        "\n",
        "        # Decoder/Upsampling path with skip connections\n",
        "        for up in self.ups:\n",
        "            skip = skips.pop()\n",
        "            x = up(x, skip, t_emb, c_emb)\n",
        "\n",
        "        # Final output\n",
        "        return self.final_conv(x)"
      ],
      "metadata": {
        "id": "hsSQQWzaYTxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MemoryEfficientWrapper:\n",
        "    \"\"\"Memory optimization utilities for 3D diffusion models.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def enable_gradient_checkpointing(model):\n",
        "        \"\"\"Enable gradient checkpointing for a model to save memory.\n",
        "\n",
        "        Args:\n",
        "            model: PyTorch model\n",
        "        \"\"\"\n",
        "        # Set all residual blocks to use checkpointing\n",
        "        for module in model.modules():\n",
        "            if hasattr(module, 'use_checkpoint'):\n",
        "                module.use_checkpoint = True\n",
        "\n",
        "        # Report memory usage\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "            logger.info(f\"CUDA memory allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
        "            logger.info(f\"CUDA memory reserved: {torch.cuda.memory_reserved()/1e9:.2f} GB\")\n",
        "\n",
        "    @staticmethod\n",
        "    def setup_mixed_precision():\n",
        "        \"\"\"Set up mixed precision training using PyTorch AMP.\n",
        "\n",
        "        Returns:\n",
        "            GradScaler for use in training loop\n",
        "        \"\"\"\n",
        "        # Return scaler for use in training loop\n",
        "        if torch.cuda.is_available():\n",
        "            return torch.cuda.amp.GradScaler()\n",
        "        else:\n",
        "            logger.warning(\"CUDA not available, mixed precision not supported\")\n",
        "            return None\n",
        "\n",
        "    @staticmethod\n",
        "    def patch_based_forward(model, x, patch_size=(32, 32, 32), overlap=4):\n",
        "        \"\"\"Process a large volume using patch-based approach with overlap.\n",
        "\n",
        "        Args:\n",
        "            model: Model to use for processing\n",
        "            x: Input volume [B, C, D, H, W]\n",
        "            patch_size: Size of patches to process\n",
        "            overlap: Overlap between patches\n",
        "\n",
        "        Returns:\n",
        "            Processed volume\n",
        "        \"\"\"\n",
        "        if x.shape[2:] <= patch_size:\n",
        "            # If volume is smaller than patch size, process directly\n",
        "            return model(x)\n",
        "\n",
        "        # Get volume dimensions\n",
        "        B, C, D, H, W = x.shape\n",
        "        pD, pH, pW = patch_size\n",
        "\n",
        "        # Calculate number of patches in each dimension\n",
        "        n_patches_d = max(1, (D - overlap) // (pD - overlap))\n",
        "        n_patches_h = max(1, (H - overlap) // (pH - overlap))\n",
        "        n_patches_w = max(1, (W - overlap) // (pW - overlap))\n",
        "\n",
        "        # Adjust patch size to cover the volume with given number of patches\n",
        "        effective_pD = (D - overlap) // n_patches_d + overlap\n",
        "        effective_pH = (H - overlap) // n_patches_h + overlap\n",
        "        effective_pW = (W - overlap) // n_patches_w + overlap\n",
        "\n",
        "        # Initialize output volume\n",
        "        output = torch.zeros_like(x)\n",
        "        count = torch.zeros_like(x)\n",
        "\n",
        "        # Process each patch\n",
        "        for i in range(n_patches_d):\n",
        "            d_start = i * (effective_pD - overlap)\n",
        "            d_end = min(d_start + effective_pD, D)\n",
        "            d_start = max(0, d_end - effective_pD)\n",
        "\n",
        "            for j in range(n_patches_h):\n",
        "                h_start = j * (effective_pH - overlap)\n",
        "                h_end = min(h_start + effective_pH, H)\n",
        "                h_start = max(0, h_end - effective_pH)\n",
        "\n",
        "                for k in range(n_patches_w):\n",
        "                    w_start = k * (effective_pW - overlap)\n",
        "                    w_end = min(w_start + effective_pW, W)\n",
        "                    w_start = max(0, w_end - effective_pW)\n",
        "\n",
        "                    # Extract patch\n",
        "                    patch = x[:, :, d_start:d_end, h_start:h_end, w_start:w_end]\n",
        "\n",
        "                    # Process patch\n",
        "                    patch_output = model(patch)\n",
        "\n",
        "                    # Create weight mask for blending (higher weight in center, lower at borders)\n",
        "                    weight = torch.ones_like(patch)\n",
        "\n",
        "                    # Update output and count\n",
        "                    output[:, :, d_start:d_end, h_start:h_end, w_start:w_end] += patch_output * weight\n",
        "                    count[:, :, d_start:d_end, h_start:h_end, w_start:w_end] += weight\n",
        "\n",
        "        # Average overlapping regions\n",
        "        output = output / count.clamp(min=1.0)\n",
        "\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def monitor_memory_usage():\n",
        "        \"\"\"Print current memory usage statistics.\n",
        "\n",
        "        Returns:\n",
        "            Allocated and reserved memory in GB\n",
        "        \"\"\"\n",
        "        if not torch.cuda.is_available():\n",
        "            logger.info(\"CUDA not available, skipping memory monitoring\")\n",
        "            return 0, 0\n",
        "\n",
        "        # Reset peak stats\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "        # Print current memory usage\n",
        "        allocated = torch.cuda.memory_allocated() / 1e9\n",
        "        reserved = torch.cuda.memory_reserved() / 1e9\n",
        "\n",
        "        logger.info(f\"CUDA memory allocated: {allocated:.2f} GB\")\n",
        "        logger.info(f\"CUDA memory reserved: {reserved:.2f} GB\")\n",
        "\n",
        "        # Return values in case needed\n",
        "        return allocated, reserved"
      ],
      "metadata": {
        "id": "TTVVyXzwYW0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_diffusion_model(\n",
        "    diffusion,\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    config=None,\n",
        "    save_dir='checkpoints',\n",
        "    resume_from=None\n",
        "):\n",
        "    \"\"\"Train the 3D diffusion model.\n",
        "\n",
        "    Args:\n",
        "        diffusion: DiffusionModel3D instance\n",
        "        model: UNet3D model\n",
        "        train_loader: Training data loader\n",
        "        val_loader: Validation data loader\n",
        "        config: Configuration object\n",
        "        save_dir: Directory to save checkpoints\n",
        "        resume_from: Optional path to resume training from checkpoint\n",
        "\n",
        "    Returns:\n",
        "        Trained model and EMA model\n",
        "    \"\"\"\n",
        "    # Use default config if not provided\n",
        "    if config is None:\n",
        "        config = BrainDiffusionConfig()\n",
        "\n",
        "    # Extract training parameters from config\n",
        "    num_epochs = config.epochs\n",
        "    learning_rate = config.learning_rate\n",
        "    weight_decay = config.weight_decay\n",
        "    mixed_precision = config.mixed_precision\n",
        "    ema_decay = config.ema_decay\n",
        "\n",
        "    # Create optimizer\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=learning_rate,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    # Create save directory if it doesn't exist\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=num_epochs, eta_min=learning_rate/10\n",
        "    )\n",
        "\n",
        "    # Set up mixed precision training if requested\n",
        "    scaler = MemoryEfficientWrapper.setup_mixed_precision() if mixed_precision else None\n",
        "\n",
        "    # Enable gradient checkpointing to save memory\n",
        "    if config.use_checkpointing:\n",
        "        MemoryEfficientWrapper.enable_gradient_checkpointing(model)\n",
        "\n",
        "    # EMA model for better generation quality\n",
        "    ema_model = copy.deepcopy(model).eval()\n",
        "\n",
        "    # Track best validation loss\n",
        "    best_val_loss = float('inf')\n",
        "    start_epoch = 0\n",
        "\n",
        "    # Resume from checkpoint if specified\n",
        "    if resume_from and os.path.exists(resume_from):\n",
        "        logger.info(f\"Loading checkpoint from {resume_from}\")\n",
        "        checkpoint = torch.load(resume_from, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "        if 'ema_model_state_dict' in checkpoint:\n",
        "            ema_model.load_state_dict(checkpoint['ema_model_state_dict'])\n",
        "\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "        if scaler is not None and 'scaler_state_dict' in checkpoint:\n",
        "            scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
        "\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
        "        logger.info(f\"Resuming from epoch {start_epoch}\")\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        # Progress bar\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        # Monitor memory before training\n",
        "        logger.info(f\"Memory before epoch {epoch+1}:\")\n",
        "        MemoryEfficientWrapper.monitor_memory_usage()\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get batch data\n",
        "            x = batch['volume'].to(device)\n",
        "            condition = batch['region'].to(device) if 'region' in batch else None\n",
        "\n",
        "            # Mixed precision training context\n",
        "            amp_context = torch.cuda.amp.autocast() if mixed_precision and torch.cuda.is_available() else nullcontext()\n",
        "\n",
        "            # Forward pass and loss calculation\n",
        "            with amp_context:\n",
        "                loss = diffusion.compute_loss(model, x, condition)\n",
        "\n",
        "            if mixed_precision and scaler is not None:\n",
        "                # Scale gradients and optimize with mixed precision\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                # Regular backprop and optimization\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "            # Update EMA model\n",
        "            with torch.no_grad():\n",
        "                for param, ema_param in zip(model.parameters(), ema_model.parameters()):\n",
        "                    ema_param.data = ema_param.data * ema_decay + param.data * (1 - ema_decay)\n",
        "\n",
        "            # Track loss\n",
        "            train_loss += loss.item()\n",
        "            progress_bar.set_postfix({\"train_loss\": loss.item()})\n",
        "\n",
        "        # Calculate average training loss\n",
        "        train_loss /= len(train_loader)"
      ],
      "metadata": {
        "id": "qXhjqYPlYYJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                x = batch['volume'].to(device)\n",
        "                condition = batch['region'].to(device) if 'region' in batch else None\n",
        "\n",
        "                # Use mixed precision if enabled\n",
        "                amp_context = torch.cuda.amp.autocast() if mixed_precision and torch.cuda.is_available() else nullcontext()\n",
        "\n",
        "                with amp_context:\n",
        "                    loss = diffusion.compute_loss(model, x, condition)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        val_loss /= max(1, len(val_loader))\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        # Log results\n",
        "        logger.info(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'ema_model_state_dict': ema_model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'train_loss': train_loss,\n",
        "            'val_loss': val_loss,\n",
        "            'best_val_loss': best_val_loss\n",
        "        }\n",
        "\n",
        "        # Add scaler state if using mixed precision\n",
        "        if scaler is not None:\n",
        "            checkpoint['scaler_state_dict'] = scaler.state_dict()\n",
        "\n",
        "        # Save latest checkpoint\n",
        "        torch.save(checkpoint, os.path.join(save_dir, 'latest_checkpoint.pt'))\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(checkpoint, os.path.join(save_dir, 'best_model.pt'))\n",
        "            logger.info(f\"Saved best model with val loss: {val_loss:.6f}\")\n",
        "\n",
        "        # Save epoch checkpoint every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0 or epoch == num_epochs - 1:\n",
        "            torch.save(checkpoint, os.path.join(save_dir, f'epoch_{epoch+1}_checkpoint.pt'))\n",
        "\n",
        "        # Generate and visualize samples every 10 epochs or at the end\n",
        "        if (epoch + 1) % 10 == 0 or epoch == num_epochs - 1:\n",
        "            generate_and_visualize_samples(diffusion, ema_model, val_loader, save_dir, epoch)\n",
        "\n",
        "        # Clear cache to prevent memory leaks\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    logger.info(\"Training completed!\")\n",
        "    return model, ema_model\n",
        "\n",
        "\n",
        "def generate_and_visualize_samples(diffusion, model, val_loader, save_dir, epoch, num_samples=2):\n",
        "    \"\"\"Generate and save sample volumes during training.\n",
        "\n",
        "    Args:\n",
        "        diffusion: DiffusionModel3D instance\n",
        "        model: UNet3D model\n",
        "        val_loader: Validation data loader\n",
        "        save_dir: Directory to save samples\n",
        "        epoch: Current epoch\n",
        "        num_samples: Number of samples to generate\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Create samples directory\n",
        "    samples_dir = os.path.join(save_dir, 'samples')\n",
        "    os.makedirs(samples_dir, exist_ok=True)\n",
        "\n",
        "    # Get a batch of validation data\n",
        "    batch = next(iter(val_loader))\n",
        "    x = batch['volume'].to(device)[:num_samples]\n",
        "    condition = batch['region'].to(device)[:num_samples] if 'region' in batch else None\n",
        "\n",
        "    # Generate samples using DDIM for efficiency\n",
        "    logger.info(\"Generating samples...\")\n",
        "    with torch.no_grad():\n",
        "        # Generate with the same conditions\n",
        "        generated, _ = diffusion.ddim_sample(\n",
        "            model,\n",
        "            x.shape,\n",
        "            condition=condition,\n",
        "            n_steps=50,\n",
        "            guidance_scale=2.0\n",
        "        )\n",
        "\n",
        "    # Convert to numpy for visualization\n",
        "    x_cpu = x.cpu().numpy()\n",
        "    generated_cpu = generated.cpu().numpy()\n",
        "\n",
        "    # Visualize middle slices from each volume\n",
        "    for i in range(num_samples):\n",
        "        real_vol = x_cpu[i, 0]\n",
        "        gen_vol = generated_cpu[i, 0]\n",
        "\n",
        "        # Get middle slices in each dimension\n",
        "        d_mid, h_mid, w_mid = [s // 2 for s in real_vol.shape]\n",
        "\n",
        "        # Create slice visualizations\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
        "\n",
        "        # Real volume slices\n",
        "        axes[0, 0].imshow(real_vol[d_mid, :, :], cmap='gray')\n",
        "        axes[0, 0].set_title(\"Real - Axial\")\n",
        "        axes[0, 0].axis('off')\n",
        "\n",
        "        axes[0, 1].imshow(real_vol[:, h_mid, :], cmap='gray')\n",
        "        axes[0, 1].set_title(\"Real - Coronal\")\n",
        "        axes[0, 1].axis('off')\n",
        "\n",
        "        axes[0, 2].imshow(real_vol[:, :, w_mid], cmap='gray')\n",
        "        axes[0, 2].set_title(\"Real - Sagittal\")\n",
        "        axes[0, 2].axis('off')\n",
        "\n",
        "        # Generated volume slices\n",
        "        axes[1, 0].imshow(gen_vol[d_mid, :, :], cmap='gray')\n",
        "        axes[1, 0].set_title(\"Generated - Axial\")\n",
        "        axes[1, 0].axis('off')\n",
        "\n",
        "        axes[1, 1].imshow(gen_vol[:, h_mid, :], cmap='gray')\n",
        "        axes[1, 1].set_title(\"Generated - Coronal\")\n",
        "        axes[1, 1].axis('off')\n",
        "\n",
        "        axes[1, 2].imshow(gen_vol[:, :, w_mid], cmap='gray')\n",
        "        axes[1, 2].set_title(\"Generated - Sagittal\")\n",
        "        axes[1, 2].axis('off')\n",
        "\n",
        "        # Save figure\n",
        "        region_label = condition[i].item() if condition is not None else \"unknown\"\n",
        "        plt.suptitle(f\"Sample {i+1}, Region: {region_label}\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(samples_dir, f'sample_{i+1}_epoch_{epoch+1}.png'))\n",
        "        plt.close()\n",
        "\n",
        "    logger.info(f\"Samples saved to {samples_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "GEvN-SflYbPA",
        "outputId": "74dad2ed-663a-4edb-f88d-cac02c223ddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 63)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m63\u001b[0m\n\u001b[0;31m    logger.info(\"Training completed!\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    }
  ]
}